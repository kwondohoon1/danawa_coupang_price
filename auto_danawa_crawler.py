# -*- coding: utf-8 -*-
"""Untitled

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-Mq30bavMMGFxHgWOOCEJpdgMXmn4L3i
"""

import os, re, io, tempfile, traceback, asyncio, aiohttp, time
from pathlib import Path
import pandas as pd
from bs4 import BeautifulSoup
from openpyxl import load_workbook
from openpyxl.styles import PatternFill
from selenium import webdriver
from selenium.webdriver.chrome.service import Service  # âœ… ì—¬ê¸°ì— ì (.) ì¤‘ìš”!
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from datetime import datetime

headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}
TARGET_COLS = ["ì¿ íŒ¡", "ì¿ íŒ¡ì™€ìš°", "ì¿ íŒ¡ì¹´ë“œí˜œíƒê°€"]

WORKDIR = Path(os.getenv("GITHUB_WORKSPACE", os.getcwd()))
INPUT_PATH = WORKDIR / "ìƒí’ˆì½”ë“œëª©ë¡.xlsx"
OUTPUT_PATH = WORKDIR / f"danawa_ì¿ íŒ¡_ê²°ê³¼_{datetime.now():%Y%m%d_%H%M}.xlsx"


def _only_digits(s):
    return re.sub(r"[^\d]", "", s or "")


def get_driver():
    from webdriver_manager.chrome import ChromeDriverManager
    options = Options()
    options.add_argument("--headless=new")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--window-size=1920,1080")
    options.add_argument("--disable-gpu")
    options.add_argument("--disable-blink-features=AutomationControlled")
    service = Service(ChromeDriverManager().install())
    return webdriver.Chrome(service=service, options=options)


def fetch_wow_prices_selenium(driver, pcodes):
    """ë‹¨ì¼ Selenium ì¸ìŠ¤í„´ìŠ¤ë¡œ ì™€ìš°ê°€ê²© ë³´ì •"""
    wow_map = {}
    for pcode in pcodes:
        try:
            url = f"https://prod.danawa.com/info/?pcode={pcode}"
            driver.get(url)
            WebDriverWait(driver, 15).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "#lowPriceCompanyArea"))
            )
            for _ in range(6):
                driver.execute_script("window.scrollBy(0, 500);")
                time.sleep(0.8)
            html = driver.page_source
            soup = BeautifulSoup(html, "html.parser")
            full_text = soup.get_text(" ", strip=True)
            wow_match = re.search(r"ì™€ìš°.?í• ì¸.?ê°€\s*([\d,]+)\s*ì›", full_text)
            if wow_match:
                wow_map[pcode] = wow_match.group(1).replace(",", "")
        except Exception:
            continue
    return wow_map


def _is_coupang_logo(el):
    if el is None:
        return False
    img = el.select_one("img[alt]")
    if img and "ì¿ íŒ¡" in (img.get("alt") or ""):
        return True
    aria = el.get("aria-label") or ""
    if "ì¿ íŒ¡" in aria:
        return True
    text_logo = el.select_one(".text__logo")
    if text_logo and "ì¿ íŒ¡" in text_logo.get_text(strip=True):
        return True
    return False


def _has_wow_hint(text):
    return "ì™€ìš°" in text or "WOW" in text.upper()


def _has_card_hint(text):
    for k in ["ì¹´ë“œ", "ì²­êµ¬", "í• ì¸", "ì‚¼ì„±", "ë¡¯ë°", "í•˜ë‚˜", "í˜„ëŒ€"]:
        if k in text:
            return True
    return False


def _pick_prices_in(el):
    prices = []
    for sel in [".text__num", ".price_num strong", ".txt_prc strong", ".prc strong"]:
        for numel in el.select(sel):
            num = _only_digits(numel.get_text(strip=True))
            if num:
                prices.append(int(num))
    return prices


def parse_product_html(html: str, pcode: str) -> dict:
    soup = BeautifulSoup(html, "html.parser")
    row = {"ìƒí’ˆì½”ë“œ": pcode, "ìƒí’ˆëª…": "ì´ë¦„ì—†ìŒ", "ì¿ íŒ¡": "X", "ì¿ íŒ¡ì™€ìš°": "X", "ì¿ íŒ¡ì¹´ë“œí˜œíƒê°€": "X"}

    tag = soup.select_one("meta[property='og:title']")
    if tag and tag.get("content"):
        row["ìƒí’ˆëª…"] = tag["content"].strip()
    else:
        alt = soup.select_one(".prod_tit, h3.tit")
        if alt:
            row["ìƒí’ˆëª…"] = alt.get_text(strip=True)

    for li in soup.select("ul.list__mall-price > li.list-item"):
        logo = li.select_one(".box__logo, .box__logo-wrap .box__logo")
        if not _is_coupang_logo(logo):
            continue
        txt = li.get_text(" ", strip=True)
        prices = _pick_prices_in(li)
        if not prices:
            continue
        price = min(prices)
        if _has_wow_hint(txt):
            row["ì¿ íŒ¡ì™€ìš°"] = str(price)
        elif _has_card_hint(txt):
            row["ì¿ íŒ¡ì¹´ë“œí˜œíƒê°€"] = str(price)
        else:
            row["ì¿ íŒ¡"] = str(price)
    return row


async def fetch_one(session, pcode):
    url = f"https://prod.danawa.com/info/?pcode={pcode}"
    try:
        async with session.get(url, timeout=10) as r:
            html = await r.text()
            return parse_product_html(html, pcode)
    except Exception:
        return {"ìƒí’ˆì½”ë“œ": pcode, "ìƒí’ˆëª…": "ì—ëŸ¬", "ì¿ íŒ¡": "X", "ì¿ íŒ¡ì™€ìš°": "X", "ì¿ íŒ¡ì¹´ë“œí˜œíƒê°€": "X"}


async def run_async_crawler(codes):
    results_map = {}
    async with aiohttp.ClientSession(headers=headers) as session:
        tasks = [fetch_one(session, c) for c in codes]
        for coro in asyncio.as_completed(tasks):
            row = await coro
            results_map[row["ìƒí’ˆì½”ë“œ"]] = row
    return results_map


def run_requests_crawler():
    try:
        print(f"[{datetime.now()}] Danawa í¬ë¡¤ë§ ì‹œì‘")
        df = pd.read_excel(INPUT_PATH)
        codes = df["ìƒí’ˆì½”ë“œ"].dropna().astype(str).tolist()

        results_map = asyncio.run(run_async_crawler(codes))

        need_wow = [c for c, r in results_map.items() if r.get("ì¿ íŒ¡ì™€ìš°") in ("X", "", None)]
        print(f"ğŸŸ¡ ì™€ìš°ê°€ê²© ë³´ì • ëŒ€ìƒ {len(need_wow)}ê°œ")
        if need_wow:
            driver = get_driver()
            wow_map = fetch_wow_prices_selenium(driver, need_wow)
            driver.quit()
            for c, w in wow_map.items():
                results_map[c]["ì¿ íŒ¡ì™€ìš°"] = w

        results = [results_map[c] for c in codes if c in results_map]
        df_out = pd.DataFrame(results).fillna("X")
        nums = pd.DataFrame({
            "ì¿ íŒ¡": pd.to_numeric(df_out["ì¿ íŒ¡"], errors="coerce"),
            "ì¿ íŒ¡ì™€ìš°": pd.to_numeric(df_out["ì¿ íŒ¡ì™€ìš°"], errors="coerce"),
            "ì¿ íŒ¡ì¹´ë“œí˜œíƒê°€": pd.to_numeric(df_out["ì¿ íŒ¡ì¹´ë“œí˜œíƒê°€"], errors="coerce"),
        })
        df_out["ìµœì €ê°€"] = nums.min(axis=1).map(lambda x: "X" if pd.isna(x) else str(int(x)))
        df_out.to_excel(OUTPUT_PATH, index=False)

        wb = load_workbook(OUTPUT_PATH)
        ws = wb.active
        yellow = PatternFill(start_color="FFFF00", end_color="FFFF00", fill_type="solid")
        header = [cell.value for cell in ws[1]]
        for idx, name in enumerate(header, start=1):
            if name in TARGET_COLS:
                for r in ws.iter_rows(min_row=1, max_row=ws.max_row, min_col=idx, max_col=idx):
                    for cell in r:
                        cell.fill = yellow
        wb.save(OUTPUT_PATH)
        print(f"âœ… ê²°ê³¼ íŒŒì¼ ìƒì„±: {OUTPUT_PATH}")

    except Exception as e:
        print(traceback.format_exc())
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")


if __name__ == "__main__":
    run_requests_crawler()
