# -*- coding: utf-8 -*-
"""Untitled

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-Mq30bavMMGFxHgWOOCEJpdgMXmn4L3i
"""

import os, re, io, tempfile, traceback, asyncio, aiohttp, time, codecs
from pathlib import Path
import pandas as pd
from bs4 import BeautifulSoup
from openpyxl import load_workbook
from openpyxl.styles import PatternFill
from selenium import webdriver
from selenium.webdriver.chrome service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from datetime import datetime

# chardet ìˆì„ ë•Œë§Œ ì‚¬ìš© (ì—†ì–´ë„ í´ë°±ìœ¼ë¡œ ë™ì‘)
try:
    import chardet  # type: ignore
except Exception:
    chardet = None

# -------------------------------------
# ê¸°ë³¸ ì„¤ì •
# -------------------------------------
headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}
TARGET_COLS = ["ì¿ íŒ¡", "ì¿ íŒ¡ì™€ìš°", "ì¿ íŒ¡ì¹´ë“œí˜œíƒê°€"]

WORKDIR = Path(os.getenv("GITHUB_WORKSPACE", os.getcwd()))
INPUT_PATH_XLSX = WORKDIR / "ìƒí’ˆì½”ë“œëª©ë¡.xlsx"
INPUT_PATH_CSV  = WORKDIR / "ìƒí’ˆì½”ë“œëª©ë¡.csv"
OUTPUT_PATH     = WORKDIR / f"danawa_ì¿ íŒ¡_ê²°ê³¼_{datetime.now():%Y%m%d_%H%M}.xlsx"

def _only_digits(s):
    return re.sub(r"[^\d]", "", s or "")

# -------------------------------------
# CSV ì¸ì½”ë”© ê°ì§€/í´ë°±
# -------------------------------------
def detect_csv_encoding(path: Path) -> str:
    # BOM ìš°ì„  ì²´í¬
    with open(path, "rb") as f:
        head = f.read(4096)
    if head.startswith(codecs.BOM_UTF8):
        return "utf-8-sig"

    if chardet:
        try:
            enc = chardet.detect(head).get("encoding") or "utf-8"
            return enc
        except Exception:
            pass

    # í´ë°±: ì‹œë„ ìˆœì„œ
    for enc in ("utf-8", "utf-8-sig", "cp949", "euc-kr"):
        try:
            head.decode(enc)
            return enc
        except Exception:
            continue
    return "cp949"

# -------------------------------------
# Chrome driver ì„¤ì •
# -------------------------------------
def get_driver():
    from webdriver_manager.chrome import ChromeDriverManager
    options = Options()
    options.add_argument("--headless=new")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--window-size=1920,1080")
    options.add_argument("--disable-gpu")
    options.add_argument("--disable-blink-features=AutomationControlled")
    service = Service(ChromeDriverManager().install())
    return webdriver.Chrome(service=service, options=options)

# -------------------------------------
# ì™€ìš°ê°€ê²© Selenium ë³´ì •
# -------------------------------------
def fetch_wow_prices_selenium(driver, pcodes):
    wow_map = {}
    for pcode in pcodes:
        try:
            url = f"https://prod.danawa.com/info/?pcode={pcode}"
            driver.get(url)
            WebDriverWait(driver, 15).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "#lowPriceCompanyArea"))
            )
            for _ in range(6):
                driver.execute_script("window.scrollBy(0, 500);")
                time.sleep(0.8)
            html = driver.page_source
            soup = BeautifulSoup(html, "html.parser")
            full_text = soup.get_text(" ", strip=True)
            wow_match = re.search(r"ì™€ìš°.?í• ì¸.?ê°€\s*([\d,]+)\s*ì›", full_text)
            if wow_match:
                wow_map[pcode] = wow_match.group(1).replace(",", "")
        except Exception:
            continue
    return wow_map

# -------------------------------------
# HTML íŒŒì‹±
# -------------------------------------
def _is_coupang_logo(el):
    if el is None:
        return False
    img = el.select_one("img[alt]")
    if img and "ì¿ íŒ¡" in (img.get("alt") or ""):
        return True
    aria = el.get("aria-label") or ""
    if "ì¿ íŒ¡" in aria:
        return True
    text_logo = el.select_one(".text__logo")
    if text_logo and "ì¿ íŒ¡" in text_logo.get_text(strip=True):
        return True
    return False

def _has_wow_hint(text):
    return "ì™€ìš°" in text or "WOW" in text.upper()

def _has_card_hint(text):
    for k in ["ì¹´ë“œ", "ì²­êµ¬", "í• ì¸", "ì‚¼ì„±", "ë¡¯ë°", "í•˜ë‚˜", "í˜„ëŒ€"]:
        if k in text:
            return True
    return False

def _pick_prices_in(el):
    prices = []
    for sel in [".text__num", ".price_num strong", ".txt_prc strong", ".prc strong"]:
        for numel in el.select(sel):
            num = _only_digits(numel.get_text(strip=True))
            if num:
                prices.append(int(num))
    return prices

def parse_product_html(html: str, pcode: str) -> dict:
    soup = BeautifulSoup(html, "html.parser")
    row = {"ìƒí’ˆì½”ë“œ": pcode, "ìƒí’ˆëª…": "ì´ë¦„ì—†ìŒ", "ì¿ íŒ¡": "X", "ì¿ íŒ¡ì™€ìš°": "X", "ì¿ íŒ¡ì¹´ë“œí˜œíƒê°€": "X"}

    tag = soup.select_one("meta[property='og:title']")
    if tag and tag.get("content"):
        row["ìƒí’ˆëª…"] = tag["content"].strip()
    else:
        alt = soup.select_one(".prod_tit, h3.tit")
        if alt:
            row["ìƒí’ˆëª…"] = alt.get_text(strip=True)

    for li in soup.select("ul.list__mall-price > li.list-item"):
        logo = li.select_one(".box__logo, .box__logo-wrap .box__logo")
        if not _is_coupang_logo(logo):
            continue
        txt = li.get_text(" ", strip=True)
        prices = _pick_prices_in(li)
        if not prices:
            continue
        price = min(prices)
        if _has_wow_hint(txt):
            row["ì¿ íŒ¡ì™€ìš°"] = str(price)
        elif _has_card_hint(txt):
            row["ì¿ íŒ¡ì¹´ë“œí˜œíƒê°€"] = str(price)
        else:
            row["ì¿ íŒ¡"] = str(price)
    return row

# -------------------------------------
# íŒŒì¼ ë¡œë“œ (XLSX/CSV ìë™)
# -------------------------------------
def read_file_safely():
    try:
        if INPUT_PATH_XLSX.exists():
            return pd.read_excel(INPUT_PATH_XLSX, engine="openpyxl")
        elif INPUT_PATH_CSV.exists():
            enc = detect_csv_encoding(INPUT_PATH_CSV)
            try:
                return pd.read_csv(INPUT_PATH_CSV, encoding=enc)
            except UnicodeDecodeError:
                # ìµœì¢… í´ë°±
                for e in ("utf-8", "utf-8-sig", "cp949", "euc-kr"):
                    try:
                        return pd.read_csv(INPUT_PATH_CSV, encoding=e)
                    except Exception:
                        continue
                raise
        else:
            raise FileNotFoundError("ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìƒí’ˆì½”ë“œëª©ë¡.xlsx ë˜ëŠ” ìƒí’ˆì½”ë“œëª©ë¡.csvê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    except Exception as e:
        raise RuntimeError(f"ì…ë ¥ íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜: {e}")

# -------------------------------------
# ë¹„ë™ê¸° ìš”ì²­ (aiohttp)
# -------------------------------------
async def fetch_one(session, pcode):
    url = f"https://prod.danawa.com/info/?pcode={pcode}"
    try:
        async with session.get(url, timeout=10) as r:
            html = await r.text()
            return parse_product_html(html, pcode)
    except Exception:
        return {"ìƒí’ˆì½”ë“œ": pcode, "ìƒí’ˆëª…": "ì—ëŸ¬", "ì¿ íŒ¡": "X", "ì¿ íŒ¡ì™€ìš°": "X", "ì¿ íŒ¡ì¹´ë“œí˜œíƒê°€": "X"}

async def run_async_crawler(codes):
    results_map = {}
    async with aiohttp.ClientSession(headers=headers) as session:
        tasks = [fetch_one(session, c) for c in codes]
        for coro in asyncio.as_completed(tasks):
            row = await coro
            results_map[row["ìƒí’ˆì½”ë“œ"]] = row
    return results_map

# -------------------------------------
# ì‹¤í–‰ í•¨ìˆ˜
# -------------------------------------
def run_requests_crawler():
    try:
        print(f"[{datetime.now()}] Danawa í¬ë¡¤ë§ ì‹œì‘")

        df = read_file_safely()
        if "ìƒí’ˆì½”ë“œ" not in df.columns:
            raise ValueError("ì…ë ¥ íŒŒì¼ì— 'ìƒí’ˆì½”ë“œ' ì—´ì´ ì—†ìŠµë‹ˆë‹¤.")
        codes = df["ìƒí’ˆì½”ë“œ"].dropna().astype(str).str.replace(r"\.0$", "", regex=True).str.strip().tolist()

        # 1ï¸âƒ£ ë¹„ë™ê¸° í¬ë¡¤ë§ (HTML ê¸°ë°˜)
        results_map = asyncio.run(run_async_crawler(codes))

        # 2ï¸âƒ£ ì™€ìš° ê°€ê²© ë¹ ì§„ í•­ëª© Seleniumìœ¼ë¡œ ë³´ì •
        need_wow = [c for c, r in results_map.items() if r.get("ì¿ íŒ¡ì™€ìš°") in ("X", "", None)]
        print(f"ğŸŸ¡ ì™€ìš°ê°€ê²© ë³´ì • ëŒ€ìƒ {len(need_wow)}ê°œ")
        if need_wow:
            driver = get_driver()
            wow_map = fetch_wow_prices_selenium(driver, need_wow)
            driver.quit()
            for c, w in wow_map.items():
                results_map[c]["ì¿ íŒ¡ì™€ìš°"] = w

        # 3ï¸âƒ£ ì—‘ì…€ ì •ë¦¬ ë° ì €ì¥
        results = [results_map[c] for c in codes if c in results_map]
        df_out = pd.DataFrame(results).fillna("X")
        nums = pd.DataFrame({
            "ì¿ íŒ¡": pd.to_numeric(df_out["ì¿ íŒ¡"], errors="coerce"),
            "ì¿ íŒ¡ì™€ìš°": pd.to_numeric(df_out["ì¿ íŒ¡ì™€ìš°"], errors="coerce"),
            "ì¿ íŒ¡ì¹´ë“œí˜œíƒê°€": pd.to_numeric(df_out["ì¿ íŒ¡ì¹´ë“œí˜œíƒê°€"], errors="coerce"),
        })
        df_out["ìµœì €ê°€"] = nums.min(axis=1).map(lambda x: "X" if pd.isna(x) else str(int(x)))
        df_out.to_excel(OUTPUT_PATH, index=False, engine="openpyxl")

        # 4ï¸âƒ£ í•˜ì´ë¼ì´íŠ¸ í‘œì‹œ
        wb = load_workbook(OUTPUT_PATH)
        ws = wb.active
        yellow = PatternFill(start_color="FFFF00", end_color="FFFF00", fill_type="solid")
        header = [cell.value for cell in ws[1]]
        for idx, name in enumerate(header, start=1):
            if name in TARGET_COLS:
                for r in ws.iter_rows(min_row=1, max_row=ws.max_row, min_col=idx, max_col=idx):
                    for cell in r:
                        cell.fill = yellow
        wb.save(OUTPUT_PATH)
        print(f"âœ… ê²°ê³¼ íŒŒì¼ ìƒì„± ì™„ë£Œ: {OUTPUT_PATH}")

    except Exception as e:
        print(traceback.format_exc())
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")

# -------------------------------------
# ë©”ì¸ ì‹¤í–‰
# -------------------------------------
if __name__ == "__main__":
    run_requests_crawler()
