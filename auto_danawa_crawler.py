# -*- coding: utf-8 -*-
"""Untitled

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-Mq30bavMMGFxHgWOOCEJpdgMXmn4L3i
"""

import os, re, io, time, traceback, requests, asyncio, aiohttp
from pathlib import Path
import pandas as pd
from bs4 import BeautifulSoup
from openpyxl import load_workbook
from openpyxl.styles import PatternFill
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from datetime import datetime

# ===============================================
# ⚡ Danawa 쿠팡 정적+동적 병합 버전
# ===============================================

headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}
TARGET_COLS = ["쿠팡", "쿠팡와우", "쿠팡카드혜택가"]

WORKDIR = Path(os.getenv("GITHUB_WORKSPACE", os.getcwd()))
INPUT_PATH = WORKDIR / "상품코드목록.xlsx"
OUTPUT_PATH = WORKDIR / f"danawa_쿠팡_정확버전_{datetime.now():%Y%m%d_%H%M}.xlsx"


# -----------------------------
# 유틸
# -----------------------------
def _only_digits(s): return re.sub(r"[^\d]", "", s or "")


# -----------------------------
# Selenium 드라이버 (동적용)
# -----------------------------
def get_driver():
    from webdriver_manager.chrome import ChromeDriverManager
    options = Options()
    options.add_argument("--headless=new")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--window-size=1920,1080")
    options.add_argument("--disable-gpu")
    service = Service(ChromeDriverManager().install())
    return webdriver.Chrome(service=service, options=options)


# -----------------------------
# 쿠팡와우가격 (동적)
# -----------------------------
def fetch_wow_prices_selenium(driver, pcodes):
    wow_map = {}
    for pcode in pcodes:
        try:
            url = f"https://prod.danawa.com/info/?pcode={pcode}"
            driver.get(url)
            WebDriverWait(driver, 15).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "#lowPriceCompanyArea"))
            )
            time.sleep(1.0)
            html = driver.page_source
            soup = BeautifulSoup(html, "html.parser")
            text = soup.get_text(" ", strip=True)
            match = re.search(r"와우.?할인.?가\s*([\d,]+)\s*원", text)
            if match:
                wow_map[pcode] = match.group(1).replace(",", "")
        except Exception:
            continue
    return wow_map


# -----------------------------
# 정적 HTML 파싱 (requests)
# -----------------------------
def parse_static_product_html(html: str, pcode: str) -> dict:
    soup = BeautifulSoup(html, "html.parser")
    row = {
        "상품코드": pcode,
        "상품명": "이름없음",
        "쿠팡": "X",
        "쿠팡와우": "X",
        "쿠팡카드혜택가": "X",
    }

    # 상품명
    tag = soup.select_one("meta[property='og:title']")
    if tag and tag.get("content"):
        row["상품명"] = tag["content"].strip()
    else:
        alt = soup.select_one(".prod_tit, h3.tit")
        if alt:
            row["상품명"] = alt.get_text(strip=True)

    # 가격들
    for li in soup.select("ul.list__mall-price > li.list-item"):
        text = li.get_text(" ", strip=True)
        logo = li.select_one(".box__logo, .box__logo-wrap .box__logo")
        if not logo or "쿠팡" not in text:
            continue
        prices = [int(_only_digits(p.get_text(strip=True))) for p in li.select(".text__num, .price_num strong, .txt_prc strong, .prc strong") if _only_digits(p.get_text(strip=True))]
        if not prices:
            continue
        price = min(prices)
        # 와우는 Selenium에서 별도 보정
        if "카드" in text or "청구" in text or "할인" in text:
            row["쿠팡카드혜택가"] = str(price)
        else:
            row["쿠팡"] = str(price)

    return row


def fetch_static_prices(codes):
    results = []
    with requests.Session() as session:
        session.headers.update(headers)
        for idx, pcode in enumerate(codes, start=1):
            try:
                url = f"https://prod.danawa.com/info/?pcode={pcode}"
                print(f"({idx}/{len(codes)}) {pcode} 정적 수집 중...")
                r = session.get(url, timeout=10)
                row = parse_static_product_html(r.text, pcode)
                results.append(row)
            except Exception as e:
                print(f"[에러] {pcode}: {e}")
                results.append({"상품코드": pcode, "상품명": "에러", "쿠팡": "X", "쿠팡와우": "X", "쿠팡카드혜택가": "X"})
    return results


# -----------------------------
# 실행
# -----------------------------
def run_crawler():
    try:
        print(f"[{datetime.now()}] Danawa 크롤링 시작")
        df = pd.read_excel(INPUT_PATH)
        codes = df["상품코드"].dropna().astype(str).str.replace(r"\.0$", "", regex=True).tolist()

        # 1️⃣ 정적 수집 (상품명, 쿠팡, 카드혜택가)
        static_results = fetch_static_prices(codes)
        results_map = {r["상품코드"]: r for r in static_results}

        # 2️⃣ 와우가격만 Selenium으로 동적 보정
        need_wow = [r["상품코드"] for r in static_results if r.get("쿠팡와우") in ("X", "", None)]
        print(f"🟡 와우가격 보정 대상 {len(need_wow)}개")

        if need_wow:
            driver = get_driver()
            wow_map = fetch_wow_prices_selenium(driver, need_wow)
            driver.quit()
            for code, wprice in wow_map.items():
                results_map[code]["쿠팡와우"] = wprice

        # 3️⃣ 최저가 계산
        df_out = pd.DataFrame(list(results_map.values())).fillna("X")
        for col in ["쿠팡", "쿠팡와우", "쿠팡카드혜택가"]:
            df_out[col] = pd.to_numeric(df_out[col], errors="coerce")
        df_out["최저가"] = df_out[["쿠팡", "쿠팡와우", "쿠팡카드혜택가"]].min(axis=1).map(lambda x: "X" if pd.isna(x) else str(int(x)))

        # 4️⃣ 엑셀 저장 + 색상 표시
        df_out.to_excel(OUTPUT_PATH, index=False, engine="openpyxl")
        wb = load_workbook(OUTPUT_PATH)
        ws = wb.active
        yellow = PatternFill(start_color="FFFF00", end_color="FFFF00", fill_type="solid")
        header = [cell.value for cell in ws[1]]
        for idx, name in enumerate(header, start=1):
            if name in TARGET_COLS:
                for r in ws.iter_rows(min_row=1, max_row=ws.max_row, min_col=idx, max_col=idx):
                    for cell in r:
                        cell.fill = yellow
        wb.save(OUTPUT_PATH)

        print(f"✅ 결과 파일 생성 완료: {OUTPUT_PATH}")

    except Exception as e:
        print(traceback.format_exc())
        print(f"❌ 오류 발생: {e}")


if __name__ == "__main__":
    run_crawler()
